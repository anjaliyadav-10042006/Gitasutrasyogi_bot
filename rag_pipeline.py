
from absl import logging
logging.set_verbosity(logging.INFO)


import pandas as pd
from sentence_transformers import SentenceTransformer, util
import faiss
import json
import google.generativeai as genai


# Configure the API key
genai.configure(api_key="AIzaSyAePz5ZXybXmND44sPSmouZhPEz7jjxX6E")

# Define the generation configuration
generation_config = {
    "temperature": 1,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192
}

# Load Data Function
def load_data():
    gita_file = "Bhagwad_Gita.csv"
    pys_file = "Patanjali_yoga_sutras.csv"
    
    gita = pd.read_csv(gita_file, encoding="utf-8")
    pys = pd.read_csv(pys_file, encoding="utf-8")

    gita_texts = gita["translation"].tolist()
    gita_chapter = gita["chapter"].tolist()
    gita_verse = gita["verse"].tolist()
    
    pys_texts = pys["translation"].tolist()
    pys_chapter = pys["chapter"].tolist()
    pys_verse = pys["verse"].tolist()
    
    return gita_texts, gita_chapter, gita_verse, pys_texts, pys_chapter, pys_verse

# Create Embeddings and Build FAISS Index
def create_faiss_index(texts, model_name="all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts, convert_to_tensor=False)
    
    d = embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(embeddings)
    
    return model, index, embeddings

# Retrieve Shlokas Function
def retrieve_shlokas(query, texts, chapter, verse, model, index, top_k=3):
    query_embedding = model.encode([query], convert_to_tensor=False)
    distances, indices = index.search(query_embedding, top_k)
    
    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            "text": texts[idx],
            "chapter": chapter[idx],
            "verse": verse[idx],
            "score": float(distances[0][i])
        })
    return results

# Generate Response Function
def GenerateResponse(query):
    gita_texts, gita_chapter, gita_verse, pys_texts, pys_chapter, pys_verse = load_data()
    
    all_texts = gita_texts + pys_texts
    all_chapter = gita_chapter + pys_chapter
    all_verse = gita_verse + pys_verse
    
    sentence_model, index, embeddings = create_faiss_index(all_texts)
    top_results = retrieve_shlokas(query, all_texts, all_chapter, all_verse, sentence_model, index)
    
    output = {
        "query": query,
        "results": top_results
    }
    
    model = genai.GenerativeModel(model_name='gemini-1.5-flash')
    response = model.generate_content(
        json.dumps(output, indent=4)
    )
     # Check for candidates and extract content 
    if hasattr(response, 'candidates') and len(response.candidates) > 0:
        try:
            response_text = response.candidates[0]['content']
        except (KeyError, IndexError, TypeError): 
            # Handle potential exceptions during content extraction
            response_text = "Error extracting response content."
    else:
        response_text = "No response generated by the language model."

    return json.dumps(output, indent=4), response_text

    return json.dumps(output, indent=4), response_text
 
# Example usage
if __name__ == "__main__":
    query = input("Enter your query: ")
    json_output, response_text = GenerateResponse(query)
    print("JSON Output:\n", json_output)
    print("Response Text:\n", response_text)
